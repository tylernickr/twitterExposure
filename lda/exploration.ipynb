{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from re import match\n",
    "from scipy.sparse import coo_matrix\n",
    "from joblib import load\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELING_DIR = './modeling/'\n",
    "LDA_DIR = './data/'\n",
    "RAW_DATA_DIR = '../twitter/tokenized_corpus/'\n",
    "\n",
    "\n",
    "def get_lda_model(ticker):\n",
    "    lda = load(LDA_DIR + ticker + '/' + ticker + '.pickle')\n",
    "    return lda\n",
    "\n",
    "\n",
    "def get_word_idx_map(filename):\n",
    "    word_idx_map = {}\n",
    "    for line in open(filename):\n",
    "        word, idx = line[:-1].split(',')\n",
    "        word_idx_map[word] = int(idx)\n",
    "    return word_idx_map\n",
    "\n",
    "\n",
    "def get_dataframe(ticker):\n",
    "    word_idx_map = get_word_idx_map(LDA_DIR + ticker + '/wordidx.dat')\n",
    "    label_idx = max([x for x in word_idx_map.values()]) + 1\n",
    "\n",
    "    records = []\n",
    "    columns = []\n",
    "    ticker_rows = 0\n",
    "    rows = 0\n",
    "    for filename in [ticker_filename, 'random.dat']:\n",
    "        for line in open(RAW_DATA_DIR + filename):\n",
    "            rows += 1\n",
    "            if filename == ticker_filename:\n",
    "                ticker_rows += 1\n",
    "            else:\n",
    "                if rows > ticker_rows * 2:\n",
    "                    break\n",
    "            line = line[:-1]\n",
    "            tokens = line.split(',')\n",
    "            data = []\n",
    "            j = []\n",
    "            word_count = {}\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    word_count[token] += 1\n",
    "                except KeyError:\n",
    "                    word_count[token] = 1\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    tok_idx = word_idx_map[token]\n",
    "                    data.append(word_count[token])\n",
    "                    j.append(tok_idx)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            data.append(1 if filename != 'random.dat' else 0)\n",
    "            j.append(label_idx)\n",
    "            columns.append(j)\n",
    "            records.append(data)\n",
    "\n",
    "    data = []\n",
    "    i = []\n",
    "    j = []\n",
    "    for row in range(len(records)):\n",
    "        data += records[row]\n",
    "        j += columns[row]\n",
    "        i += [row] * len(records[row])\n",
    "    wc_sparse_vector = coo_matrix((data, (i, j)))\n",
    "    wc_data = pd.DataFrame(data=wc_sparse_vector.toarray())\n",
    "    return wc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA\n",
      "Naive Bayes\n",
      "0.8503113681783768\n",
      "Logistic\n",
      "0.8516021990164448\n",
      "ADABoost\n",
      "0.8486515336502718\n",
      "Random Forest\n",
      "0.8514207665182012\n",
      "WMT\n",
      "Naive Bayes\n",
      "0.8291510963829938\n",
      "Logistic\n",
      "0.8298821862524888\n",
      "ADABoost\n",
      "0.8289662674630595\n",
      "Random Forest\n",
      "0.8296990360693386\n",
      "DIS\n",
      "Naive Bayes\n",
      "0.9151\n",
      "Logistic\n",
      "0.9153\n",
      "ADABoost\n",
      "0.9167000000000002\n",
      "Random Forest\n",
      "0.9153499999999999\n",
      "JPM\n",
      "Naive Bayes\n",
      "0.7826325848064979\n",
      "Logistic\n",
      "0.7826206402293359\n",
      "ADABoost\n",
      "0.7727902532250359\n",
      "Random Forest\n",
      "0.7815695174390827\n",
      "CAT\n",
      "Naive Bayes\n",
      "0.8678801986343887\n",
      "Logistic\n",
      "0.8673215394165116\n",
      "ADABoost\n",
      "0.8600837988826816\n",
      "Random Forest\n",
      "0.8639726877715704\n",
      "JNJ\n",
      "Naive Bayes\n",
      "0.6265151515151515\n",
      "Logistic\n",
      "0.625\n",
      "ADABoost\n",
      "0.6174242424242424\n",
      "Random Forest\n",
      "0.6\n",
      "AXP\n",
      "Naive Bayes\n",
      "0.8198795180722891\n",
      "Logistic\n",
      "0.8271084337349398\n",
      "ADABoost\n",
      "0.8216867469879519\n",
      "Random Forest\n",
      "0.819277108433735\n",
      "KO\n",
      "Naive Bayes\n",
      "0.8444214876033058\n",
      "Logistic\n",
      "0.8429752066115703\n",
      "ADABoost\n",
      "0.8413223140495868\n",
      "Random Forest\n",
      "0.846900826446281\n",
      "GS\n",
      "Naive Bayes\n",
      "0.7126984126984126\n",
      "Logistic\n",
      "0.7063492063492063\n",
      "ADABoost\n",
      "0.7190476190476189\n",
      "Random Forest\n",
      "0.719047619047619\n",
      "PFE\n",
      "Naive Bayes\n",
      "0.8078224101479915\n",
      "Logistic\n",
      "0.8101479915433405\n",
      "ADABoost\n",
      "0.8100951374207188\n",
      "Random Forest\n",
      "0.8054968287526428\n",
      "DOW\n",
      "Naive Bayes\n",
      "0.7066666666666667\n",
      "Logistic\n",
      "0.6699999999999999\n",
      "ADABoost\n",
      "0.6900000000000001\n",
      "Random Forest\n",
      "0.6733333333333332\n",
      "AAPL\n",
      "Naive Bayes\n",
      "0.9046673078994181\n",
      "Logistic\n",
      "0.9057120840188212\n",
      "ADABoost\n",
      "0.9045677558958133\n",
      "Random Forest\n",
      "0.9045180170310427\n",
      "UNH\n",
      "Naive Bayes\n",
      "0.7714285714285715\n",
      "Logistic\n",
      "0.7714285714285715\n",
      "ADABoost\n",
      "0.7714285714285715\n",
      "Random Forest\n",
      "0.7857142857142858\n",
      "MMM\n",
      "Naive Bayes\n",
      "0.7775287031808771\n",
      "Logistic\n",
      "0.7810050818746471\n",
      "ADABoost\n",
      "0.7758008658008657\n",
      "Random Forest\n",
      "0.7797044984001504\n",
      "MRK\n",
      "Naive Bayes\n",
      "0.8951818181818181\n",
      "Logistic\n",
      "0.8941818181818182\n",
      "ADABoost\n",
      "0.8951919191919192\n",
      "Random Forest\n",
      "0.8941818181818182\n",
      "TRV\n",
      "Naive Bayes\n",
      "0.75\n",
      "Logistic\n",
      "0.75\n",
      "ADABoost\n",
      "0.75\n",
      "Random Forest\n",
      "0.75\n",
      "XOM\n",
      "Naive Bayes\n",
      "0.77625\n",
      "Logistic\n",
      "0.7825\n",
      "ADABoost\n",
      "0.77625\n",
      "Random Forest\n",
      "0.7775000000000001\n",
      "PG\n",
      "Naive Bayes\n",
      "0.5833333333333333\n",
      "Logistic\n",
      "0.55\n",
      "ADABoost\n",
      "0.55\n",
      "Random Forest\n",
      "0.55\n",
      "WBA\n",
      "Naive Bayes\n",
      "0.6727272727272726\n",
      "Logistic\n",
      "0.666488413547237\n",
      "ADABoost\n",
      "0.6517825311942959\n",
      "Random Forest\n",
      "0.6755793226381461\n",
      "INTC\n",
      "Naive Bayes\n",
      "0.8260994717787404\n",
      "Logistic\n",
      "0.8264445961184641\n",
      "ADABoost\n",
      "0.8273069618374977\n",
      "Random Forest\n",
      "0.8266176034186005\n",
      "HD\n",
      "Naive Bayes\n",
      "0.7804347826086956\n",
      "Logistic\n",
      "0.7839130434782609\n",
      "ADABoost\n",
      "0.7821739130434783\n",
      "Random Forest\n",
      "0.7847826086956522\n",
      "MSFT\n",
      "Naive Bayes\n",
      "0.8531518322330497\n",
      "Logistic\n",
      "0.8520085675358734\n",
      "ADABoost\n",
      "0.8529033479955466\n",
      "Random Forest\n",
      "0.8541460780432988\n",
      "UTX\n",
      "Naive Bayes\n",
      "0.7899999999999999\n",
      "Logistic\n",
      "0.765\n",
      "ADABoost\n",
      "0.8099999999999999\n",
      "Random Forest\n",
      "0.74\n",
      "CSCO\n",
      "Naive Bayes\n",
      "0.8050215186444758\n",
      "Logistic\n",
      "0.8043513809050873\n",
      "ADABoost\n",
      "0.806143096477764\n",
      "Random Forest\n",
      "0.8047983065980479\n",
      "MCD\n",
      "Naive Bayes\n",
      "0.7226327944572748\n",
      "Logistic\n",
      "0.7224018475750577\n",
      "ADABoost\n",
      "0.7226327944572748\n",
      "Random Forest\n",
      "0.7184757505773671\n",
      "V\n",
      "Naive Bayes\n",
      "0.8785624238571959\n",
      "Logistic\n",
      "0.876258276391758\n",
      "ADABoost\n",
      "0.8783304200434345\n",
      "Random Forest\n",
      "0.8801710895704222\n",
      "IBM\n",
      "Naive Bayes\n",
      "0.8308996050717106\n",
      "Logistic\n",
      "0.8306622323841196\n",
      "ADABoost\n",
      "0.8319581514584632\n",
      "Random Forest\n",
      "0.8298375944017182\n",
      "VZ\n",
      "Naive Bayes\n",
      "0.6773234811165846\n",
      "Logistic\n",
      "0.6790541871921183\n",
      "ADABoost\n",
      "0.6733333333333333\n",
      "Random Forest\n",
      "0.6750574712643679\n",
      "NKE\n",
      "Naive Bayes\n",
      "0.8732109486821733\n",
      "Logistic\n",
      "0.8730611485753066\n",
      "ADABoost\n",
      "0.8736598513693318\n",
      "Random Forest\n",
      "0.8726123951781808\n",
      "CVX\n",
      "Naive Bayes\n",
      "0.8469252271139064\n",
      "Logistic\n",
      "0.8320754716981131\n",
      "ADABoost\n",
      "0.8243186582809224\n",
      "Random Forest\n",
      "0.8281621243885395\n"
     ]
    }
   ],
   "source": [
    "model_scores = {}\n",
    "model_dict = {}\n",
    "for ticker_filename in [x for x in os.listdir(RAW_DATA_DIR) if x != 'random.dat']:\n",
    "    ticker = match('(.*)\\.dat', ticker_filename).group(1)\n",
    "    raw_data = get_dataframe(ticker)\n",
    "    raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "    raw_input = raw_data[raw_data.columns[:-1]].values\n",
    "    labels = raw_data[raw_data.columns[-1]].values\n",
    "    lda = get_lda_model(ticker)\n",
    "    clean_data = lda.transform(raw_input)\n",
    "    models = [\n",
    "        ('Naive Bayes', GaussianNB()),\n",
    "        ('Logistic', LogisticRegression(solver='lbfgs')),\n",
    "        ('ADABoost', AdaBoostClassifier()),\n",
    "        ('Random Forest', RandomForestClassifier(n_estimators=100))\n",
    "    ]\n",
    "    print(ticker)\n",
    "    for model_name, model in models:\n",
    "        results = cross_validate(mymodel,\n",
    "                             clean_data,\n",
    "                             labels,\n",
    "                             return_estimator=True,\n",
    "                             cv=5)\n",
    "        print(model_name)\n",
    "        print(np.mean(results['test_score']))\n",
    "        ret_model = results['estimator'][0]\n",
    "        if model_name == 'Logistic':\n",
    "            model_dict[ticker] = ret_model\n",
    "        try:\n",
    "            model_scores[model_name].append(np.mean(results['test_score']))\n",
    "        except KeyError:\n",
    "            model_scores[model_name] = [np.mean(results['test_score'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "0.8530178895301146\n",
      "Logistic\n",
      "0.8524433207740411\n",
      "ADABoost\n",
      "0.8489344682944097\n",
      "Random Forest\n",
      "0.851697642550215\n"
     ]
    }
   ],
   "source": [
    "for model, scores in model_scores.items():\n",
    "    print(model)\n",
    "    print(np.mean([x for x in scores if x >= .8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../twitter/processed_tweet_data/'\n",
    "proc_data = {}\n",
    "for datafile in os.listdir(DATA_DIR):\n",
    "    ticker = match('(.*)\\.dat', datafile).group(1)\n",
    "    tweet_data = pd.read_csv(DATA_DIR + datafile, header=None).values\n",
    "    ticker_model = model_dict[ticker]\n",
    "    dated_data = {}\n",
    "    tweet_dates = tweet_data[:,-1]\n",
    "    tweet_data = tweet_data[:,:-1]\n",
    "    preds = ticker_model.predict_proba(tweet_data)[:,1].reshape(-1, 1)\n",
    "    mod_data = tweet_data * preds\n",
    "    final_data = []\n",
    "    for i in range(len(tweet_data)):\n",
    "        tdate = str(tweet_dates[i])[:8]\n",
    "        try:\n",
    "            dated_data[tdate].append(mod_data[i])\n",
    "        except KeyError:\n",
    "            dated_data[tdate] = [mod_data[i]]\n",
    "    for key in dated_data.keys():\n",
    "        dated_data[key] = np.array(dated_data[key]).sum(0)\n",
    "        final_data.append([key] + list(dated_data[key]))\n",
    "    proc_data[ticker] = final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA\n",
      "life: 20.4371\n",
      "720: 9.7903\n",
      "anniversari: 9.01\n",
      "promis: 9.01\n",
      "one: 8.9519\n",
      "get: 8.6884\n",
      "candid: 8.01\n",
      "suck: 8.01\n",
      "sit: 7.01\n",
      "ram: 6.01\n",
      "\n",
      "\n",
      "WMT\n",
      "vote: 126.9048\n",
      "dog: 94.01\n",
      "hot: 75.01\n",
      "popular: 57.01\n",
      "retweet: 37.48\n",
      "room: 25.2643\n",
      "repli: 12.9933\n",
      "respect: 12.7317\n",
      "check: 9.5048\n",
      "mickey: 8.01\n",
      "\n",
      "\n",
      "DIS\n",
      "one: 738.9738\n",
      "song: 119.6681\n",
      "student: 42.7314\n",
      "perform: 31.9433\n",
      "red: 28.6973\n",
      "make: 23.4708\n",
      "best: 20.7862\n",
      "show: 17.5114\n",
      "mama: 15.7537\n",
      "want: 14.7091\n",
      "\n",
      "\n",
      "JPM\n",
      "vehicl: 6.01\n",
      "track: 5.8213\n",
      "explor: 2.01\n",
      "use: 2.01\n",
      "news: 1.01\n",
      "lend: 1.01\n",
      "sourc: 1.01\n",
      "fed: 1.01\n",
      "check: 1.01\n",
      "thread: 1.01\n",
      "\n",
      "\n",
      "CAT\n",
      "old: 4.01\n",
      "becam: 4.01\n",
      "fashion: 4.01\n",
      "part: 3.01\n",
      "impeach: 3.01\n",
      "commun: 2.01\n",
      "call: 2.01\n",
      "report: 2.01\n",
      "show: 2.01\n",
      "hous: 2.01\n",
      "\n",
      "\n",
      "JNJ\n",
      "world: 2.01\n",
      "better: 1.01\n",
      "present: 1.01\n",
      "global: 0.01\n",
      "medic: 0.01\n",
      "box: 0.01\n",
      "market: 0.01\n",
      "research: 0.01\n",
      "report: 0.01\n",
      "20122024: 0.01\n",
      "\n",
      "\n",
      "AXP\n",
      "hand: 3.01\n",
      "visa: 2.01\n",
      "thing: 2.01\n",
      "puppi: 2.01\n",
      "dick: 2.01\n",
      "time: 1.01\n",
      "get: 1.01\n",
      "payment: 1.01\n",
      "new: 1.01\n",
      "say: 1.01\n",
      "\n",
      "\n",
      "KO\n",
      "plastic: 14.4967\n",
      "doctor: 14.01\n",
      "pollut: 12.0854\n",
      "world: 7.9427\n",
      "driver: 7.01\n",
      "kick: 7.0087\n",
      "understand: 6.992\n",
      "largest: 6.9906\n",
      "year: 6.8823\n",
      "run: 5.8824\n",
      "\n",
      "\n",
      "GS\n",
      "goal: 9.01\n",
      "back: 4.01\n",
      "player: 4.01\n",
      "vs: 4.01\n",
      "midfield: 4.01\n",
      "west: 1.01\n",
      "help: 1.01\n",
      "cut: 1.01\n",
      "forc: 1.01\n",
      "citi: 1.01\n",
      "\n",
      "\n",
      "PFE\n",
      "inhibitor: 0.01\n",
      "result: 0.01\n",
      "roll: 0.01\n",
      "pretti: 0.01\n",
      "promis: 0.01\n",
      "excit: 0.01\n",
      "earli: 0.01\n",
      "stage: 0.01\n",
      "clinic: 0.01\n",
      "support: 0.01\n",
      "\n",
      "\n",
      "AAPL\n",
      "chang: 215.1751\n",
      "stay: 67.6109\n",
      "liter: 64.3626\n",
      "abus: 59.2931\n",
      "appl: 50.0041\n",
      "demand: 46.5532\n",
      "least: 44.9169\n",
      "map: 42.4936\n",
      "crazi: 40.9087\n",
      "thing: 36.4496\n",
      "\n",
      "\n",
      "UNH\n",
      "knew: 0.01\n",
      "foreign: 0.01\n",
      "firm: 0.01\n",
      "bid: 0.01\n",
      "contract: 0.01\n",
      "subsidiari: 0.01\n",
      "us: 0.01\n",
      "compani: 0.01\n",
      "among: 0.01\n",
      "privat: 0.01\n",
      "\n",
      "\n",
      "MMM\n",
      "pretti: 15.01\n",
      "dinner: 5.01\n",
      "taco: 4.01\n",
      "oh: 4.01\n",
      "feel: 3.01\n",
      "like: 2.8929\n",
      "look: 2.01\n",
      "hot: 2.01\n",
      "tri: 2.01\n",
      "put: 2.01\n",
      "\n",
      "\n",
      "MRK\n",
      "use: 4.01\n",
      "deni: 4.01\n",
      "lie: 4.01\n",
      "lee: 4.01\n",
      "peopl: 2.01\n",
      "room: 2.01\n",
      "yet: 2.01\n",
      "one: 2.01\n",
      "best: 2.01\n",
      "forc: 1.01\n",
      "\n",
      "\n",
      "TRV\n",
      "uptrend: 0.01\n",
      "stochast: 0.01\n",
      "indic: 0.01\n",
      "remain: 0.01\n",
      "overbought: 0.01\n",
      "zone: 0.01\n",
      "11: 0.01\n",
      "day: 0.01\n",
      "view: 0.01\n",
      "odd: 0.01\n",
      "\n",
      "\n",
      "PG\n",
      "counsel: 0.01\n",
      "brand: 0.01\n",
      "protect: 0.01\n",
      "exhibit: 0.01\n",
      "citi: 0.01\n",
      "center: 0.01\n",
      "panama: 0.01\n",
      "volunt: 0.01\n",
      "busi: 0.01\n",
      "today: 0.01\n",
      "\n",
      "\n",
      "WBA\n",
      "boycott: 116.01\n",
      "patient: 7.4469\n",
      "deni: 1.0146\n",
      "medic: 1.01\n",
      "commit: 1.01\n",
      "malpractic: 1.01\n",
      "violat: 1.01\n",
      "civil: 1.01\n",
      "right: 1.01\n",
      "legal: 1.01\n",
      "\n",
      "\n",
      "INTC\n",
      "call: 28.4645\n",
      "nine: 26.01\n",
      "ten: 14.01\n",
      "eighti: 13.01\n",
      "matter: 11.01\n",
      "presid: 8.1185\n",
      "zero: 8.01\n",
      "fact: 7.8751\n",
      "trump: 7.2427\n",
      "well: 6.9313\n",
      "\n",
      "\n",
      "HD\n",
      "read: 9.01\n",
      "report: 8.6961\n",
      "point: 6.01\n",
      "cruelti: 4.01\n",
      "patio: 4.01\n",
      "paramount: 4.01\n",
      "trump: 4.01\n",
      "come: 3.01\n",
      "network: 3.01\n",
      "agre: 3.01\n",
      "\n",
      "\n",
      "MSFT\n",
      "perform: 132.7256\n",
      "test: 123.696\n",
      "bib: 112.01\n",
      "vs: 104.5779\n",
      "super: 72.9249\n",
      "without: 52.2632\n",
      "packag: 43.0121\n",
      "recruit: 40.5052\n",
      "window: 38.626\n",
      "ran: 38.01\n",
      "\n",
      "\n",
      "UTX\n",
      "enter: 0.01\n",
      "uptrend: 0.01\n",
      "momentum: 0.01\n",
      "indic: 0.01\n",
      "exceed: 0.01\n",
      "level: 0.01\n",
      "25: 0.01\n",
      "2019: 0.01\n",
      "view: 0.01\n",
      "odd: 0.01\n",
      "\n",
      "\n",
      "CSCO\n",
      "like: 34.0461\n",
      "guy: 23.7774\n",
      "rose: 19.01\n",
      "look: 15.56\n",
      "make: 14.1328\n",
      "bitch: 14.01\n",
      "help: 12.3753\n",
      "find: 11.8844\n",
      "beauti: 10.01\n",
      "busi: 8.0721\n",
      "\n",
      "\n",
      "MCD\n",
      "phone: 20.3999\n",
      "get: 20.2722\n",
      "nugget: 13.8088\n",
      "journalist: 9.9487\n",
      "work: 9.7339\n",
      "attorney: 9.01\n",
      "record: 8.1359\n",
      "100: 6.6008\n",
      "call: 6.4526\n",
      "chicken: 6.1607\n",
      "\n",
      "\n",
      "V\n",
      "skill: 32.01\n",
      "visa: 16.6066\n",
      "got: 11.01\n",
      "call: 9.9432\n",
      "subclass: 9.01\n",
      "nomin: 9.01\n",
      "long: 8.8983\n",
      "period: 8.01\n",
      "time: 7.559\n",
      "migrat: 7.1604\n",
      "\n",
      "\n",
      "IBM\n",
      "point: 58.3573\n",
      "around: 22.1209\n",
      "drake: 22.01\n",
      "best: 18.0934\n",
      "sorri: 14.9825\n",
      "rank: 14.724\n",
      "buy: 13.933\n",
      "read: 13.4372\n",
      "other: 12.9205\n",
      "blame: 12.01\n",
      "\n",
      "\n",
      "VZ\n",
      "hold: 57.0558\n",
      "point: 9.01\n",
      "promis: 6.01\n",
      "call: 5.2824\n",
      "inform: 5.01\n",
      "cruelti: 4.01\n",
      "huge: 4.01\n",
      "keep: 4.01\n",
      "comput: 4.01\n",
      "import: 4.01\n",
      "\n",
      "\n",
      "NKE\n",
      "know: 482.2391\n",
      "famili: 204.8999\n",
      "tweet: 185.2534\n",
      "campaign: 96.9929\n",
      "nigga: 65.4758\n",
      "question: 46.246\n",
      "plan: 45.2368\n",
      "answer: 40.9731\n",
      "best: 30.873\n",
      "worth: 30.2661\n",
      "\n",
      "\n",
      "CVX\n",
      "whoever: 1.01\n",
      "rob: 1.01\n",
      "deserv: 1.01\n",
      "veteran: 1.01\n",
      "discount: 1.01\n",
      "troop: 1.01\n",
      "guy: 1.01\n",
      "smartest: 1.01\n",
      "clubhous: 1.01\n",
      "sale: 0.01\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "STK_DATA_DIR = '../stocks/stock_data/'\n",
    "for datafile in os.listdir(STK_DATA_DIR):\n",
    "    ticker = match('(.*)\\.dat', datafile).group(1)\n",
    "    stk_data = pd.read_csv(STK_DATA_DIR + datafile, delimiter='|').values\n",
    "    stk_dates = stk_data[:,0]\n",
    "    tik_dates = []\n",
    "    for tdate in stk_dates:\n",
    "        tik_dates.append(tdate[:4] + tdate[5:7] + tdate[8:10])\n",
    "    tik_dates = np.array(tik_dates).reshape(-1, 1)\n",
    "    stk_data = np.concatenate((stk_data, tik_dates), 1)\n",
    "    stk_data = pd.DataFrame(data=stk_data[:,[4, 2]])\n",
    "    tweet_data = pd.DataFrame(data=proc_data[ticker])\n",
    "    \n",
    "    all_data = pd.merge(tweet_data, stk_data, on=tweet_data.columns[0])\n",
    "    all_data['1_y'] = abs(all_data['1_y'])\n",
    "    labels = all_data['1_y'].values\n",
    "    input_data = all_data.drop(columns=[0, '1_y']).values\n",
    "    coefs = LinearRegression(normalize=True).fit(input_data, labels).coef_\n",
    "    max_vector = list(coefs).index(max(coefs))\n",
    "    vector_comps = get_lda_model(ticker).components_[max_vector]\n",
    "    word_idx_map = get_word_idx_map(LDA_DIR + ticker + '/wordidx.dat')\n",
    "    word_scores = []\n",
    "    for word, idx in sorted(word_idx_map.items(), key=lambda x: x[1]):\n",
    "        word_scores.append((word, vector_comps[idx]))\n",
    "    print(ticker)\n",
    "    for word, score in sorted(word_scores, key=lambda x: -x[1])[:10]:\n",
    "        print(word + ': ' + str(round(score, 4)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.13571485e-01 6.25769885e-02 4.20055599e-02 ... 1.86722266e-02\n",
      "  1.95092219e-01 6.28951989e-02]\n",
      " [1.78000886e+00 8.47687160e-01 5.76358518e-01 ... 5.88824217e-01\n",
      "  5.61517512e-01 8.74036243e-01]\n",
      " [4.61342217e-01 3.06137280e-01 1.33130300e-01 ... 3.10580776e-02\n",
      "  3.32608919e-01 9.08080776e-02]\n",
      " ...\n",
      " [4.00000000e-04 4.00000000e-04 4.00000000e-04 ... 4.00000000e-04\n",
      "  4.00000000e-04 4.00000000e-04]\n",
      " [1.00000000e-04 1.00000000e-04 1.00000000e-04 ... 1.00000000e-04\n",
      "  1.00000000e-04 1.00000000e-04]\n",
      " [7.98885862e-03 6.00000000e-05 6.00000000e-05 ... 6.00000000e-05\n",
      "  6.00000000e-05 6.00000000e-05]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
