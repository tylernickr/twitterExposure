{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from re import match\n",
    "from scipy.sparse import coo_matrix\n",
    "from joblib import load\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELING_DIR = './modeling/'\n",
    "LDA_DIR = './data/'\n",
    "RAW_DATA_DIR = '../twitter/tokenized_corpus/'\n",
    "\n",
    "\n",
    "def get_lda_model(ticker):\n",
    "    lda = load(LDA_DIR + ticker + '/' + ticker + '.pickle')\n",
    "    return lda\n",
    "\n",
    "\n",
    "def get_word_idx_map(filename):\n",
    "    word_idx_map = {}\n",
    "    for line in open(filename):\n",
    "        word, idx = line[:-1].split(',')\n",
    "        word_idx_map[word] = int(idx)\n",
    "    return word_idx_map\n",
    "\n",
    "\n",
    "def get_dataframe(ticker):\n",
    "    word_idx_map = get_word_idx_map(LDA_DIR + ticker + '/wordidx.dat')\n",
    "    label_idx = max([x for x in word_idx_map.values()]) + 1\n",
    "\n",
    "    records = []\n",
    "    columns = []\n",
    "    ticker_rows = 0\n",
    "    rows = 0\n",
    "    for filename in [ticker_filename, 'random.dat']:\n",
    "        for line in open(RAW_DATA_DIR + filename):\n",
    "            rows += 1\n",
    "            if filename == ticker_filename:\n",
    "                ticker_rows += 1\n",
    "            else:\n",
    "                if rows > ticker_rows * 2:\n",
    "                    break\n",
    "            line = line[:-1]\n",
    "            tokens = line.split(',')\n",
    "            data = []\n",
    "            j = []\n",
    "            word_count = {}\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    word_count[token] += 1\n",
    "                except KeyError:\n",
    "                    word_count[token] = 1\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    tok_idx = word_idx_map[token]\n",
    "                    data.append(word_count[token])\n",
    "                    j.append(tok_idx)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            data.append(1 if filename != 'random.dat' else 0)\n",
    "            j.append(label_idx)\n",
    "            columns.append(j)\n",
    "            records.append(data)\n",
    "\n",
    "    data = []\n",
    "    i = []\n",
    "    j = []\n",
    "    for row in range(len(records)):\n",
    "        data += records[row]\n",
    "        j += columns[row]\n",
    "        i += [row] * len(records[row])\n",
    "    wc_sparse_vector = coo_matrix((data, (i, j)))\n",
    "    wc_data = pd.DataFrame(data=wc_sparse_vector.toarray())\n",
    "    return wc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA\n",
      "Naive Bayes\n",
      "0.7456694927392897\n",
      "Logistic\n",
      "0.7753886134055425\n",
      "ADABoost\n",
      "0.7783379146175935\n",
      "Random Forest\n",
      "0.8506895799087382\n",
      "WMT\n",
      "Naive Bayes\n",
      "0.7174798467649064\n",
      "Logistic\n",
      "0.7328682393072861\n",
      "ADABoost\n",
      "0.7501818071937228\n",
      "Random Forest\n",
      "0.8292401533693926\n",
      "DIS\n",
      "Naive Bayes\n",
      "0.79485\n",
      "Logistic\n",
      "0.8530999999999999\n",
      "ADABoost\n",
      "0.8657499999999999\n",
      "Random Forest\n",
      "0.9151999999999999\n",
      "JPM\n",
      "Naive Bayes\n",
      "0.6854634495938844\n",
      "Logistic\n",
      "0.7368609651218346\n",
      "ADABoost\n",
      "0.7543717152412804\n",
      "Random Forest\n",
      "0.7859173435260391\n",
      "CAT\n",
      "Naive Bayes\n",
      "0.7770328988206083\n",
      "Logistic\n",
      "0.8171818746120423\n",
      "ADABoost\n",
      "0.8489447548106768\n",
      "Random Forest\n",
      "0.8511421477343266\n",
      "JNJ\n",
      "Naive Bayes\n",
      "0.6265151515151516\n",
      "Logistic\n",
      "0.6439393939393939\n",
      "ADABoost\n",
      "0.6787878787878788\n",
      "Random Forest\n",
      "0.6522727272727271\n",
      "AXP\n",
      "Naive Bayes\n",
      "0.7054216867469879\n",
      "Logistic\n",
      "0.7740963855421688\n",
      "ADABoost\n",
      "0.7734939759036145\n",
      "Random Forest\n",
      "0.8150602409638555\n",
      "KO\n",
      "Naive Bayes\n",
      "0.6830578512396694\n",
      "Logistic\n",
      "0.7495867768595041\n",
      "ADABoost\n",
      "0.7904958677685949\n",
      "Random Forest\n",
      "0.8421487603305785\n",
      "GS\n",
      "Naive Bayes\n",
      "0.580952380952381\n",
      "Logistic\n",
      "0.6\n",
      "ADABoost\n",
      "0.6476190476190476\n",
      "Random Forest\n",
      "0.7222222222222222\n",
      "PFE\n",
      "Naive Bayes\n",
      "0.7499471458773784\n",
      "Logistic\n",
      "0.7941331923890064\n",
      "ADABoost\n",
      "0.7988372093023257\n",
      "Random Forest\n",
      "0.8009513742071881\n",
      "DOW\n",
      "Naive Bayes\n",
      "0.8033333333333333\n",
      "Logistic\n",
      "0.8233333333333333\n",
      "ADABoost\n",
      "0.7133333333333334\n",
      "Random Forest\n",
      "0.7166666666666668\n",
      "AAPL\n",
      "Naive Bayes\n",
      "0.8119215690643573\n",
      "Logistic\n",
      "0.8242110794621071\n",
      "ADABoost\n",
      "0.8223697759027703\n",
      "Random Forest\n",
      "0.9060104924494224\n",
      "UNH\n",
      "Naive Bayes\n",
      "0.8\n",
      "Logistic\n",
      "0.8\n",
      "ADABoost\n",
      "0.7428571428571429\n",
      "Random Forest\n",
      "0.7857142857142857\n",
      "MMM\n",
      "Naive Bayes\n",
      "0.689493694711086\n",
      "Logistic\n",
      "0.7077206851119895\n",
      "ADABoost\n",
      "0.725501599849426\n",
      "Random Forest\n",
      "0.7744946357989836\n",
      "MRK\n",
      "Naive Bayes\n",
      "0.7884040404040403\n",
      "Logistic\n",
      "0.8942121212121211\n",
      "ADABoost\n",
      "0.8922121212121212\n",
      "Random Forest\n",
      "0.9002424242424242\n",
      "TRV\n",
      "Naive Bayes\n",
      "0.9\n",
      "Logistic\n",
      "0.95\n",
      "ADABoost\n",
      "0.75\n",
      "Random Forest\n",
      "0.75\n",
      "XOM\n",
      "Naive Bayes\n",
      "0.63625\n",
      "Logistic\n",
      "0.7374999999999999\n",
      "ADABoost\n",
      "0.75875\n",
      "Random Forest\n",
      "0.7825\n",
      "PG\n",
      "Naive Bayes\n",
      "0.7666666666666667\n",
      "Logistic\n",
      "0.8166666666666668\n",
      "ADABoost\n",
      "0.5666666666666667\n",
      "Random Forest\n",
      "0.55\n",
      "WBA\n",
      "Naive Bayes\n",
      "0.5721033868092691\n",
      "Logistic\n",
      "0.6311942959001782\n",
      "ADABoost\n",
      "0.6340463458110517\n",
      "Random Forest\n",
      "0.6369875222816399\n",
      "INTC\n",
      "Naive Bayes\n",
      "0.7105730310404178\n",
      "Logistic\n",
      "0.7155626446673393\n",
      "ADABoost\n",
      "0.7274363463707044\n",
      "Random Forest\n",
      "0.8242088551249334\n",
      "HD\n",
      "Naive Bayes\n",
      "0.6434782608695653\n",
      "Logistic\n",
      "0.6595652173913044\n",
      "ADABoost\n",
      "0.668695652173913\n",
      "Random Forest\n",
      "0.7804347826086957\n",
      "MSFT\n",
      "Naive Bayes\n",
      "0.7238018199527666\n",
      "Logistic\n",
      "0.7463214622283697\n",
      "ADABoost\n",
      "0.7613841312058052\n",
      "Random Forest\n",
      "0.8532016699208181\n",
      "UTX\n",
      "Naive Bayes\n",
      "0.9400000000000001\n",
      "Logistic\n",
      "0.8949999999999999\n",
      "ADABoost\n",
      "0.845\n",
      "Random Forest\n",
      "0.8150000000000001\n",
      "CSCO\n",
      "Naive Bayes\n",
      "0.6830985844845056\n",
      "Logistic\n",
      "0.7086505953993238\n",
      "ADABoost\n",
      "0.714032764518815\n",
      "Random Forest\n",
      "0.8021107332390326\n",
      "MCD\n",
      "Naive Bayes\n",
      "0.6369515011547343\n",
      "Logistic\n",
      "0.6609699769053118\n",
      "ADABoost\n",
      "0.6831408775981525\n",
      "Random Forest\n",
      "0.7219399538106235\n",
      "V\n",
      "Naive Bayes\n",
      "0.74908151914826\n",
      "Logistic\n",
      "0.8123205678266856\n",
      "ADABoost\n",
      "0.8578536998781715\n",
      "Random Forest\n",
      "0.8801562582763918\n",
      "IBM\n",
      "Naive Bayes\n",
      "0.699222753412319\n",
      "Logistic\n",
      "0.710045867110095\n",
      "ADABoost\n",
      "0.7459365343310468\n",
      "Random Forest\n",
      "0.8354901960784312\n",
      "VZ\n",
      "Naive Bayes\n",
      "0.5806929392446634\n",
      "Logistic\n",
      "0.6046830870279146\n",
      "ADABoost\n",
      "0.6498817733990149\n",
      "Random Forest\n",
      "0.6773628899835796\n",
      "NKE\n",
      "Naive Bayes\n",
      "0.7803385400305671\n",
      "Logistic\n",
      "0.7939552341396447\n",
      "ADABoost\n",
      "0.7944038631284099\n",
      "Random Forest\n",
      "0.8751557872592153\n",
      "CVX\n",
      "Naive Bayes\n",
      "0.660377358490566\n",
      "Logistic\n",
      "0.7593640810621942\n",
      "ADABoost\n",
      "0.8078266946191475\n",
      "Random Forest\n",
      "0.8266946191474493\n"
     ]
    }
   ],
   "source": [
    "model_scores = {}\n",
    "model_dict = {}\n",
    "for ticker_filename in [x for x in os.listdir(RAW_DATA_DIR) if x != 'random.dat']:\n",
    "    ticker = match('(.*)\\.dat', ticker_filename).group(1)\n",
    "    raw_data = get_dataframe(ticker)\n",
    "    raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "    raw_input = raw_data[raw_data.columns[:-1]].values\n",
    "    labels = raw_data[raw_data.columns[-1]].values\n",
    "    lda = get_lda_model(ticker)\n",
    "    clean_data = lda.transform(raw_input)\n",
    "    models = [\n",
    "        ('Naive Bayes', GaussianNB()),\n",
    "        ('Logistic', LogisticRegression(solver='lbfgs')),\n",
    "        ('ADABoost', AdaBoostClassifier()),\n",
    "        ('Random Forest', RandomForestClassifier(n_estimators=100))\n",
    "    ]\n",
    "    print(ticker)\n",
    "    for model_name, model in models:\n",
    "        results = cross_validate(model,\n",
    "                             clean_data,\n",
    "                             labels,\n",
    "                             return_estimator=True,\n",
    "                             cv=5)\n",
    "        print(model_name)\n",
    "        print(np.mean(results['test_score']))\n",
    "        ret_model = results['estimator'][0]\n",
    "        if model_name == 'Logistic':\n",
    "            model_dict[ticker] = ret_model\n",
    "        try:\n",
    "            model_scores[model_name].append(np.mean(results['test_score']))\n",
    "        except KeyError:\n",
    "            model_scores[model_name] = [np.mean(results['test_score'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "0.8510509804795381\n",
      "Logistic\n",
      "0.8486025643112954\n",
      "ADABoost\n",
      "0.8485652923461268\n",
      "Random Forest\n",
      "0.8483943113089528\n"
     ]
    }
   ],
   "source": [
    "for model, scores in model_scores.items():\n",
    "    print(model)\n",
    "    print(np.mean([x for x in scores if x >= .8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../twitter/processed_tweet_data/'\n",
    "proc_data = {}\n",
    "for datafile in os.listdir(DATA_DIR):\n",
    "    ticker = match('(.*)\\.dat', datafile).group(1)\n",
    "    tweet_data = pd.read_csv(DATA_DIR + datafile, header=None).values\n",
    "    ticker_model = model_dict[ticker]\n",
    "    dated_data = {}\n",
    "    tweet_dates = tweet_data[:,-1]\n",
    "    tweet_data = tweet_data[:,:-1]\n",
    "    preds = ticker_model.predict_proba(tweet_data)[:,1].reshape(-1, 1)\n",
    "    mod_data = tweet_data * preds\n",
    "    final_data = []\n",
    "    for i in range(len(tweet_data)):\n",
    "        tdate = str(tweet_dates[i])[:8]\n",
    "        try:\n",
    "            dated_data[tdate].append(mod_data[i])\n",
    "        except KeyError:\n",
    "            dated_data[tdate] = [mod_data[i]]\n",
    "    for key in dated_data.keys():\n",
    "        dated_data[key] = np.array(dated_data[key]).sum(0)\n",
    "        final_data.append([key] + list(dated_data[key]))\n",
    "    proc_data[ticker] = final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA\n",
      "cockpit: 59.9735\n",
      "film: 44.5506\n",
      "seen: 17.8834\n",
      "airbu: 15.5023\n",
      "save: 13.209\n",
      "aviat: 13.1904\n",
      "air: 13.1527\n",
      "30: 13.01\n",
      "hope: 12.1458\n",
      "giveaway: 11.01\n",
      "\n",
      "\n",
      "WMT\n",
      "walk: 59.6552\n",
      "know: 49.1175\n",
      "support: 46.0081\n",
      "anyon: 40.5524\n",
      "pic: 26.8303\n",
      "race: 19.8468\n",
      "site: 17.7868\n",
      "smile: 16.7154\n",
      "reveal: 15.9311\n",
      "believ: 13.5009\n",
      "\n",
      "\n",
      "DIS\n",
      "kid: 300.3857\n",
      "start: 154.3203\n",
      "thought: 135.9112\n",
      "hate: 119.5582\n",
      "fight: 103.6547\n",
      "need: 96.3888\n",
      "special: 74.6914\n",
      "class: 63.6087\n",
      "take: 45.2403\n",
      "one: 44.6747\n",
      "\n",
      "\n",
      "JPM\n",
      "vehicl: 6.01\n",
      "track: 5.8213\n",
      "explor: 2.01\n",
      "use: 2.01\n",
      "news: 1.01\n",
      "lend: 1.01\n",
      "sourc: 1.01\n",
      "fed: 1.01\n",
      "check: 1.01\n",
      "thread: 1.01\n",
      "\n",
      "\n",
      "CAT\n",
      "educ: 4.01\n",
      "toy: 4.01\n",
      "rattl: 4.01\n",
      "start: 2.01\n",
      "ask: 2.01\n",
      "question: 2.01\n",
      "manag: 2.01\n",
      "march: 2.01\n",
      "cat: 1.081\n",
      "money: 1.0737\n",
      "\n",
      "\n",
      "JNJ\n",
      "must: 1.01\n",
      "forgot: 1.01\n",
      "famou: 1.01\n",
      "global: 0.01\n",
      "medic: 0.01\n",
      "box: 0.01\n",
      "market: 0.01\n",
      "research: 0.01\n",
      "report: 0.01\n",
      "20122024: 0.01\n",
      "\n",
      "\n",
      "AXP\n",
      "decor: 55.5299\n",
      "art: 52.2292\n",
      "wall: 31.8453\n",
      "photographi: 12.01\n",
      "artistri: 8.01\n",
      "natur: 6.9395\n",
      "sale: 4.9813\n",
      "pack: 4.01\n",
      "owe: 4.01\n",
      "50: 4.01\n",
      "\n",
      "\n",
      "KO\n",
      "tri: 57.1703\n",
      "know: 13.1625\n",
      "send: 12.9005\n",
      "anyth: 10.9344\n",
      "say: 9.8258\n",
      "someon: 8.3861\n",
      "help: 7.64\n",
      "work: 7.4239\n",
      "still: 7.2824\n",
      "get: 6.6312\n",
      "\n",
      "\n",
      "GS\n",
      "page: 1.01\n",
      "presid: 1.01\n",
      "use: 1.01\n",
      "claim: 1.01\n",
      "messag: 1.01\n",
      "taken: 1.01\n",
      "context: 1.01\n",
      "vilifi: 1.01\n",
      "gs: 1.01\n",
      "stock: 1.01\n",
      "\n",
      "\n",
      "PFE\n",
      "inhibitor: 0.01\n",
      "result: 0.01\n",
      "roll: 0.01\n",
      "pretti: 0.01\n",
      "promis: 0.01\n",
      "excit: 0.01\n",
      "earli: 0.01\n",
      "stage: 0.01\n",
      "clinic: 0.01\n",
      "support: 0.01\n",
      "\n",
      "\n",
      "AAPL\n",
      "smart: 77.592\n",
      "super: 70.9252\n",
      "basic: 62.01\n",
      "guid: 58.3927\n",
      "tutori: 55.955\n",
      "appl: 55.9448\n",
      "provid: 55.8717\n",
      "offici: 52.7613\n",
      "other: 44.6511\n",
      "user: 28.8308\n",
      "\n",
      "\n",
      "UNH\n",
      "knew: 0.01\n",
      "foreign: 0.01\n",
      "firm: 0.01\n",
      "bid: 0.01\n",
      "contract: 0.01\n",
      "subsidiari: 0.01\n",
      "us: 0.01\n",
      "compani: 0.01\n",
      "among: 0.01\n",
      "privat: 0.01\n",
      "\n",
      "\n",
      "MMM\n",
      "villag: 4.01\n",
      "tonight: 4.01\n",
      "2019: 3.01\n",
      "much: 3.01\n",
      "famili: 3.01\n",
      "must: 3.01\n",
      "serv: 2.01\n",
      "come: 2.01\n",
      "miss: 2.01\n",
      "littl: 2.01\n",
      "\n",
      "\n",
      "MRK\n",
      "get: 4.01\n",
      "children: 2.01\n",
      "end: 1.01\n",
      "best: 1.01\n",
      "take: 1.01\n",
      "advantag: 1.01\n",
      "deal: 1.01\n",
      "buy: 1.01\n",
      "crazi: 1.01\n",
      "game: 1.01\n",
      "\n",
      "\n",
      "TRV\n",
      "uptrend: 0.01\n",
      "stochast: 0.01\n",
      "indic: 0.01\n",
      "remain: 0.01\n",
      "overbought: 0.01\n",
      "zone: 0.01\n",
      "11: 0.01\n",
      "day: 0.01\n",
      "view: 0.01\n",
      "odd: 0.01\n",
      "\n",
      "\n",
      "PG\n",
      "counsel: 0.01\n",
      "brand: 0.01\n",
      "protect: 0.01\n",
      "exhibit: 0.01\n",
      "citi: 0.01\n",
      "center: 0.01\n",
      "panama: 0.01\n",
      "volunt: 0.01\n",
      "busi: 0.01\n",
      "today: 0.01\n",
      "\n",
      "\n",
      "WBA\n",
      "order: 1.01\n",
      "today: 1.01\n",
      "pictur: 1.01\n",
      "give: 1.01\n",
      "famili: 1.01\n",
      "friend: 1.01\n",
      "annual: 0.01\n",
      "well: 0.01\n",
      "visit: 0.01\n",
      "avail: 0.01\n",
      "\n",
      "\n",
      "INTC\n",
      "right: 28.3866\n",
      "hp: 27.01\n",
      "human: 20.739\n",
      "ass: 20.3239\n",
      "million: 14.7995\n",
      "like: 14.0819\n",
      "idea: 11.5184\n",
      "good: 11.2101\n",
      "hit: 10.9605\n",
      "time: 10.2188\n",
      "\n",
      "\n",
      "HD\n",
      "good: 60.8432\n",
      "look: 47.995\n",
      "homeown: 26.01\n",
      "amazon: 20.01\n",
      "10: 18.2547\n",
      "11: 10.01\n",
      "broken: 10.01\n",
      "19: 5.1532\n",
      "lo: 5.01\n",
      "order: 4.01\n",
      "\n",
      "\n",
      "MSFT\n",
      "amaz: 146.2394\n",
      "tonight: 145.01\n",
      "god: 144.4404\n",
      "connect: 132.9711\n",
      "peopl: 91.2609\n",
      "new: 85.7596\n",
      "look: 72.8957\n",
      "amazon: 65.3706\n",
      "googl: 59.0134\n",
      "anon: 52.01\n",
      "\n",
      "\n",
      "UTX\n",
      "enter: 0.01\n",
      "uptrend: 0.01\n",
      "momentum: 0.01\n",
      "indic: 0.01\n",
      "exceed: 0.01\n",
      "level: 0.01\n",
      "25: 0.01\n",
      "2019: 0.01\n",
      "view: 0.01\n",
      "odd: 0.01\n",
      "\n",
      "\n",
      "CSCO\n",
      "read: 25.2474\n",
      "articl: 20.9761\n",
      "90: 20.01\n",
      "left: 15.5367\n",
      "day: 14.9221\n",
      "let: 13.755\n",
      "packet: 12.4719\n",
      "solut: 11.6594\n",
      "tracer: 11.6247\n",
      "made: 11.4154\n",
      "\n",
      "\n",
      "MCD\n",
      "gt: 49.8879\n",
      "find: 28.8407\n",
      "home: 19.7975\n",
      "interest: 16.9641\n",
      "radon: 16.01\n",
      "yo: 9.01\n",
      "min: 6.01\n",
      "track: 6.01\n",
      "cut: 6.01\n",
      "fine: 5.01\n",
      "\n",
      "\n",
      "V\n",
      "act: 12.01\n",
      "hold: 7.9631\n",
      "peopl: 6.4858\n",
      "job: 5.9875\n",
      "like: 5.3556\n",
      "messag: 5.01\n",
      "hurt: 4.0881\n",
      "relat: 4.01\n",
      "mascot: 4.01\n",
      "equal: 4.01\n",
      "\n",
      "\n",
      "IBM\n",
      "time: 327.8997\n",
      "wrong: 35.4301\n",
      "work: 27.0903\n",
      "mean: 26.6325\n",
      "program: 25.9791\n",
      "place: 23.5472\n",
      "turn: 21.7843\n",
      "real: 21.5829\n",
      "end: 20.8984\n",
      "right: 17.5052\n",
      "\n",
      "\n",
      "VZ\n",
      "phone: 10.9553\n",
      "worst: 10.01\n",
      "line: 3.01\n",
      "trick: 3.01\n",
      "happi: 2.01\n",
      "us: 2.01\n",
      "god: 2.01\n",
      "thank: 2.01\n",
      "thanksgiv: 2.01\n",
      "lie: 2.01\n",
      "\n",
      "\n",
      "NKE\n",
      "watch: 282.7475\n",
      "2020: 160.4521\n",
      "may: 146.9526\n",
      "done: 128.4818\n",
      "past: 94.3627\n",
      "marvel: 66.01\n",
      "studio: 54.5888\n",
      "new: 53.9279\n",
      "trailer: 46.01\n",
      "okay: 40.9588\n",
      "\n",
      "\n",
      "CVX\n",
      "whoever: 1.01\n",
      "rob: 1.01\n",
      "deserv: 1.01\n",
      "veteran: 1.01\n",
      "discount: 1.01\n",
      "troop: 1.01\n",
      "guy: 1.01\n",
      "smartest: 1.01\n",
      "clubhous: 1.01\n",
      "sale: 0.01\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "STK_DATA_DIR = '../stocks/stock_data/'\n",
    "for datafile in os.listdir(STK_DATA_DIR):\n",
    "    ticker = match('(.*)\\.dat', datafile).group(1)\n",
    "    stk_data = pd.read_csv(STK_DATA_DIR + datafile, delimiter='|').values\n",
    "    stk_dates = stk_data[:,0]\n",
    "    tik_dates = []\n",
    "    for tdate in stk_dates:\n",
    "        tik_dates.append(tdate[:4] + tdate[5:7] + tdate[8:10])\n",
    "    tik_dates = np.array(tik_dates).reshape(-1, 1)\n",
    "    stk_data = np.concatenate((stk_data, tik_dates), 1)\n",
    "    stk_data = pd.DataFrame(data=stk_data[:,[4, 2]])\n",
    "    tweet_data = pd.DataFrame(data=proc_data[ticker])\n",
    "    \n",
    "    all_data = pd.merge(tweet_data, stk_data, on=tweet_data.columns[0])\n",
    "    all_data['1_y'] = abs(all_data['1_y'])\n",
    "    labels = all_data['1_y'].values\n",
    "    input_data = all_data.drop(columns=[0, '1_y']).values\n",
    "    coefs = LinearRegression(normalize=True).fit(input_data, labels).coef_\n",
    "    max_vector = list(coefs).index(max(coefs))\n",
    "    vector_comps = get_lda_model(ticker).components_[max_vector]\n",
    "    word_idx_map = get_word_idx_map(LDA_DIR + ticker + '/wordidx.dat')\n",
    "    word_scores = []\n",
    "    for word, idx in sorted(word_idx_map.items(), key=lambda x: x[1]):\n",
    "        word_scores.append((word, vector_comps[idx]))\n",
    "    print(ticker)\n",
    "    for word, score in sorted(word_scores, key=lambda x: -x[1])[:10]:\n",
    "        print(word + ': ' + str(round(score, 4)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
